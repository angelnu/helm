# Default values for esphome.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

replicaCount: 1

image:
  repository: esphome/esphome
  pullPolicy: IfNotPresent

persistence:
  enabled: false
  existingClaim: ""
clusterfs:
  enabled: false
  #pvcName: esphome
  accessModes:

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

# Enable pod security context (must be `true` if runAsUser or fsGroup are set)
usePodSecurityContext: true
# Set runAsUser to 1000 to let home-assistant run as non-root user 'hass' which exists in 'runningman84/alpine-homeassistant' docker image.
# When setting runAsUser to a different value than 0 also set fsGroup to the same value:
# runAsUser: <defaults to 0>
# fsGroup: <will be omitted in deployment if runAsUser is 0>

git:
  enabled: false

  ## we just use the hass-configurator container image
  ## you can use any image which has git and openssh installed
  ##
  image:
    repository: causticlab/hass-configurator-docker
    tag: 0.3.5-x86_64
    pullPolicy: IfNotPresent

vscode:
  enabled: false

  ## code-server container image
  ##
  image:
    repository: codercom/code-server
    tag: 3.0.1
    pullPolicy: IfNotPresent

  ## VSCode password
  # password:

  ## path where the home assistant configuration is stored
  hassConfig: /config

  ## path where the VS Code data should reside
  vscodePath: /config/.vscode

  ## Additional hass-vscode container environment variable
  ## For instance to add a http_proxy
  ##
  extraEnv: {}

  ingress:
    enabled: false
    annotations:
      # kubernetes.io/ingress.class: nginx
      # kubernetes.io/tls-acme: "true"
      nginx.ingress.kubernetes.io/proxy-body-size: 8000m
    path: /
    hosts:
      - esphome-editor.local
    tls: []
    #  - secretName: home-assistant-tls
    #    hosts:
    #      - home-assistant.local
  service:
    type: ClusterIP
    port: 8080
    annotations: {}
    labels: {}
    clusterIP: ""
    ## List of IP addresses at which the hass-vscode service is available
    ## Ref: https://kubernetes.io/docs/user-guide/services/#external-ips
    ##
    externalIPs: []
    loadBalancerIP: ""
    loadBalancerSourceRanges: []
    # nodePort: 30000

serviceAccount:
  # Specifies whether a service account should be created
  create: true
  # Annotations to add to the service account
  annotations: {}
  # The name of the service account to use.
  # If not set and create is true, a name is generated using the fullname template
  name:

## Add support for Prometheus
# settings has to be enabled in configuration.yaml
# https://www.home-assistant.io/components/prometheus/
monitoring:
  enabled: false
  serviceMonitor:
    # When set true and if Prometheus Operator is installed then use a ServiceMonitor to configure scraping
    enabled: true
    # Set the namespace the ServiceMonitor should be deployed
    # namespace: monitoring
    # Set how frequently Prometheus should scrape
    # interval: 30s
    # Set path to beats-exporter telemtery-path
    # telemetryPath: /metrics
    # Set labels for the ServiceMonitor, use this to define your scrape label for Prometheus Operator
    # labels:
podSecurityContext: {}
  # fsGroup: 2000

securityContext: {}
  # capabilities:
  #   drop:
  #   - ALL
  # readOnlyRootFilesystem: true
  # runAsNonRoot: true
  # runAsUser: 1000

service:
  type: ClusterIP
  port: 6052

ingress:
  enabled: false
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
    - host: chart-example.local
      paths: []
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local
  path: /

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
  # limits:
  #   cpu: 100m
  #   memory: 128Mi
  # requests:
  #   cpu: 100m
  #   memory: 128Mi

nodeSelector: {}

tolerations: []

affinity: {}
